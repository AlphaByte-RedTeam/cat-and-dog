{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  # for accessing the file system\n",
    "from itertools import product  # for efficient looping process\n",
    "\n",
    "import numpy as np  # for linear algebra\n",
    "import pandas as pd  # for data analysis\n",
    "# for image processing\n",
    "import PIL\n",
    "# for deep learning process\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing import image\n",
    "from keras.utils import image_dataset_from_directory\n",
    "from PIL import Image\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find the count of the files in the datasets directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images found: 10028 files belonging\n"
     ]
    }
   ],
   "source": [
    "set_list = [\"test_set\", \"training_set\"]\n",
    "cats_dogs = [\"cats\", \"dogs\"]\n",
    "folderpath = [\n",
    "    os.path.abspath(f\"datasets/{sets}/{animals}\")\n",
    "    for sets, animals in product(set_list, cats_dogs)\n",
    "]\n",
    "\n",
    "count = sum(len(os.listdir(folderpath[i])) for i in range(len(folderpath)))\n",
    "print(f\"Total images found: {count} files belonging\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing all variables needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 260\n",
    "epoch_amounts = 50\n",
    "labels = \"inferred\"\n",
    "label_mode = \"categorical\"\n",
    "interpolation = \"nearest\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the data\n",
    "Before feeding the data to the model, we need to convert the images files to a float so the computer won't break the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_float(img, label):\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8005 files belonging to 2 classes.\n",
      "Found 2023 files belonging to 2 classes.\n",
      "<BatchDataset element_spec=(TensorSpec(shape=(None, 260, 260, 3), dtype=tf.uint8, name=None), TensorSpec(shape=(None, 2), dtype=tf.float32, name=None))> <BatchDataset element_spec=(TensorSpec(shape=(None, 260, 260, 3), dtype=tf.uint8, name=None), TensorSpec(shape=(None, 2), dtype=tf.float32, name=None))>\n"
     ]
    }
   ],
   "source": [
    "train_ds = image_dataset_from_directory(\n",
    "    \"datasets/training_set\",\n",
    "    seed=1,\n",
    "    labels=labels,\n",
    "    label_mode=label_mode,\n",
    "    color_mode=\"rgb\",\n",
    "    batch_size=32,\n",
    "    image_size=(260, 260),\n",
    "    interpolation=interpolation,\n",
    "    shuffle=True,\n",
    ")\n",
    "test_ds = image_dataset_from_directory(\n",
    "    \"datasets/test_set\",\n",
    "    seed=1,\n",
    "    labels=labels,\n",
    "    label_mode=label_mode,\n",
    "    color_mode=\"rgb\",\n",
    "    batch_size=32,\n",
    "    image_size=(260, 260),\n",
    "    interpolation=interpolation,\n",
    "    shuffle=True,\n",
    ")\n",
    "print(train_ds, test_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cats = list(train_ds.take(1))[0][0][0]\n",
    "# Image.open(cats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the model using the Sequential API\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assumptions\n",
    "\n",
    "Now we have training data and train data. In test data, we have 8005 images.\n",
    "And in the test data, we have 2023 images. We have 2 classes, which are cats and dogs.\n",
    "I assume that the total images available is enough for training and validating the model.\n",
    "\n",
    "Now let's jump into creating the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    # Input layer\n",
    "    keras.layers.InputLayer(input_shape=(size, size, 3)),\n",
    "\n",
    "    keras.layers.BatchNormalization(renorm=True),\n",
    "    keras.layers.Conv2D(filters=64, kernel_size=3, activation='relu', padding='same'),\n",
    "    keras.layers.MaxPooling2D(),\n",
    "    \n",
    "    keras.layers.BatchNormalization(renorm=True),\n",
    "    keras.layers.Conv2D(filters=128, kernel_size=3, activation='relu', padding='same'),\n",
    "    keras.layers.MaxPooling2D(),\n",
    "\n",
    "    keras.layers.BatchNormalization(renorm=True),\n",
    "    keras.layers.Conv2D(filters=256, kernel_size=3, activation='relu', padding='same'),\n",
    "    keras.layers.MaxPooling2D(),\n",
    "\n",
    "    keras.layers.BatchNormalization(renorm=True),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(8, activation='relu'),\n",
    "    keras.layers.Dense(2, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam_opt = keras.optimizers.Adam(epsilon=0.01)\n",
    "model.compile(\n",
    "    optimizer=adam_opt,\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['categorical_accuracy'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "251/251 [==============================] - 667s 3s/step - loss: 2.4733 - categorical_accuracy: 0.5429 - val_loss: 0.6751 - val_categorical_accuracy: 0.6134\n",
      "Epoch 2/50\n",
      "251/251 [==============================] - 723s 3s/step - loss: 0.7998 - categorical_accuracy: 0.5634 - val_loss: 0.6787 - val_categorical_accuracy: 0.6169\n",
      "Epoch 3/50\n",
      "251/251 [==============================] - 788s 3s/step - loss: 0.6851 - categorical_accuracy: 0.6057 - val_loss: 0.7101 - val_categorical_accuracy: 0.6214\n",
      "Epoch 4/50\n",
      "251/251 [==============================] - 788s 3s/step - loss: 0.6015 - categorical_accuracy: 0.6598 - val_loss: 0.6282 - val_categorical_accuracy: 0.6535\n",
      "Epoch 5/50\n",
      "251/251 [==============================] - 789s 3s/step - loss: 0.5607 - categorical_accuracy: 0.6962 - val_loss: 0.6043 - val_categorical_accuracy: 0.6634\n",
      "Epoch 6/50\n",
      "251/251 [==============================] - 675s 3s/step - loss: 0.5252 - categorical_accuracy: 0.7187 - val_loss: 0.5835 - val_categorical_accuracy: 0.6767\n",
      "Epoch 7/50\n",
      "251/251 [==============================] - 693s 3s/step - loss: 0.4828 - categorical_accuracy: 0.7484 - val_loss: 0.6324 - val_categorical_accuracy: 0.6619\n",
      "Epoch 8/50\n",
      "251/251 [==============================] - 685s 3s/step - loss: 0.4567 - categorical_accuracy: 0.7597 - val_loss: 0.6273 - val_categorical_accuracy: 0.6881\n",
      "Epoch 9/50\n",
      "251/251 [==============================] - 679s 3s/step - loss: 0.4293 - categorical_accuracy: 0.7821 - val_loss: 0.6384 - val_categorical_accuracy: 0.6945\n",
      "Epoch 10/50\n",
      "251/251 [==============================] - 685s 3s/step - loss: 0.4902 - categorical_accuracy: 0.7529 - val_loss: 0.6601 - val_categorical_accuracy: 0.6920\n",
      "Epoch 11/50\n",
      "251/251 [==============================] - 653s 3s/step - loss: 0.4146 - categorical_accuracy: 0.7905 - val_loss: 0.6551 - val_categorical_accuracy: 0.7173\n",
      "Epoch 12/50\n",
      "251/251 [==============================] - 622s 2s/step - loss: 0.3654 - categorical_accuracy: 0.8170 - val_loss: 0.7120 - val_categorical_accuracy: 0.7029\n",
      "Epoch 13/50\n",
      "251/251 [==============================] - 662s 3s/step - loss: 0.3227 - categorical_accuracy: 0.8456 - val_loss: 0.7270 - val_categorical_accuracy: 0.7261\n",
      "Epoch 14/50\n",
      "251/251 [==============================] - 679s 3s/step - loss: 0.3072 - categorical_accuracy: 0.8510 - val_loss: 0.8437 - val_categorical_accuracy: 0.7108\n",
      "Epoch 15/50\n",
      "251/251 [==============================] - 705s 3s/step - loss: 0.2780 - categorical_accuracy: 0.8652 - val_loss: 0.8037 - val_categorical_accuracy: 0.7182\n",
      "Epoch 16/50\n",
      "251/251 [==============================] - 727s 3s/step - loss: 0.2454 - categorical_accuracy: 0.8862 - val_loss: 0.8600 - val_categorical_accuracy: 0.7311\n",
      "Epoch 17/50\n",
      "251/251 [==============================] - 675s 3s/step - loss: 0.2393 - categorical_accuracy: 0.8952 - val_loss: 0.9504 - val_categorical_accuracy: 0.7064\n",
      "Epoch 18/50\n",
      "251/251 [==============================] - 637s 3s/step - loss: 0.2346 - categorical_accuracy: 0.8903 - val_loss: 0.9718 - val_categorical_accuracy: 0.7153\n",
      "Epoch 19/50\n",
      "251/251 [==============================] - 635s 3s/step - loss: 0.1830 - categorical_accuracy: 0.9148 - val_loss: 1.1213 - val_categorical_accuracy: 0.7252\n",
      "Epoch 20/50\n",
      "251/251 [==============================] - 635s 3s/step - loss: 0.1526 - categorical_accuracy: 0.9315 - val_loss: 1.1442 - val_categorical_accuracy: 0.7271\n",
      "Epoch 21/50\n",
      "251/251 [==============================] - 637s 3s/step - loss: 0.1369 - categorical_accuracy: 0.9393 - val_loss: 1.2452 - val_categorical_accuracy: 0.7405\n",
      "Epoch 22/50\n",
      "251/251 [==============================] - 636s 3s/step - loss: 0.1285 - categorical_accuracy: 0.9408 - val_loss: 1.3694 - val_categorical_accuracy: 0.7311\n",
      "Epoch 23/50\n",
      "251/251 [==============================] - 638s 3s/step - loss: 0.1137 - categorical_accuracy: 0.9453 - val_loss: 1.2878 - val_categorical_accuracy: 0.7182\n",
      "Epoch 24/50\n",
      "251/251 [==============================] - 671s 3s/step - loss: 0.1098 - categorical_accuracy: 0.9494 - val_loss: 1.4192 - val_categorical_accuracy: 0.7306\n",
      "Epoch 25/50\n",
      "251/251 [==============================] - 665s 3s/step - loss: 0.0984 - categorical_accuracy: 0.9520 - val_loss: 1.5184 - val_categorical_accuracy: 0.7281\n",
      "Epoch 26/50\n",
      "251/251 [==============================] - 681s 3s/step - loss: 0.0950 - categorical_accuracy: 0.9552 - val_loss: 1.4581 - val_categorical_accuracy: 0.7266\n",
      "Epoch 27/50\n",
      "251/251 [==============================] - 767s 3s/step - loss: 0.0871 - categorical_accuracy: 0.9585 - val_loss: 1.4805 - val_categorical_accuracy: 0.7435\n",
      "Epoch 28/50\n",
      "251/251 [==============================] - 830s 3s/step - loss: 0.0757 - categorical_accuracy: 0.9636 - val_loss: 1.6702 - val_categorical_accuracy: 0.7449\n",
      "Epoch 29/50\n",
      "251/251 [==============================] - 813s 3s/step - loss: 0.0727 - categorical_accuracy: 0.9668 - val_loss: 1.6133 - val_categorical_accuracy: 0.7331\n",
      "Epoch 30/50\n",
      "251/251 [==============================] - 782s 3s/step - loss: 0.0894 - categorical_accuracy: 0.9621 - val_loss: 1.4950 - val_categorical_accuracy: 0.7400\n",
      "Epoch 31/50\n",
      "251/251 [==============================] - 789s 3s/step - loss: 0.0580 - categorical_accuracy: 0.9723 - val_loss: 1.6433 - val_categorical_accuracy: 0.7341\n",
      "Epoch 32/50\n",
      "251/251 [==============================] - 775s 3s/step - loss: 0.0643 - categorical_accuracy: 0.9705 - val_loss: 1.7212 - val_categorical_accuracy: 0.7435\n",
      "Epoch 33/50\n",
      "251/251 [==============================] - 799s 3s/step - loss: 0.0621 - categorical_accuracy: 0.9704 - val_loss: 1.7730 - val_categorical_accuracy: 0.7252\n",
      "Epoch 34/50\n",
      "251/251 [==============================] - 687s 3s/step - loss: 0.0665 - categorical_accuracy: 0.9689 - val_loss: 1.6411 - val_categorical_accuracy: 0.7380\n",
      "Epoch 35/50\n",
      "251/251 [==============================] - 640s 3s/step - loss: 0.0535 - categorical_accuracy: 0.9755 - val_loss: 1.6956 - val_categorical_accuracy: 0.7355\n",
      "Epoch 36/50\n",
      "251/251 [==============================] - 637s 3s/step - loss: 0.0505 - categorical_accuracy: 0.9745 - val_loss: 1.7058 - val_categorical_accuracy: 0.7380\n",
      "Epoch 37/50\n",
      "251/251 [==============================] - 650s 3s/step - loss: 0.0494 - categorical_accuracy: 0.9763 - val_loss: 1.7720 - val_categorical_accuracy: 0.7375\n",
      "Epoch 38/50\n",
      "251/251 [==============================] - 637s 3s/step - loss: 0.0391 - categorical_accuracy: 0.9810 - val_loss: 1.8324 - val_categorical_accuracy: 0.7459\n",
      "Epoch 39/50\n",
      "251/251 [==============================] - 637s 3s/step - loss: 0.0340 - categorical_accuracy: 0.9821 - val_loss: 1.9228 - val_categorical_accuracy: 0.7435\n",
      "Epoch 40/50\n",
      "251/251 [==============================] - 637s 3s/step - loss: 0.0355 - categorical_accuracy: 0.9814 - val_loss: 1.9314 - val_categorical_accuracy: 0.7336\n",
      "Epoch 41/50\n",
      "251/251 [==============================] - 638s 3s/step - loss: 0.0365 - categorical_accuracy: 0.9821 - val_loss: 2.0839 - val_categorical_accuracy: 0.7410\n",
      "Epoch 42/50\n",
      "251/251 [==============================] - 638s 3s/step - loss: 0.0284 - categorical_accuracy: 0.9844 - val_loss: 2.0909 - val_categorical_accuracy: 0.7459\n",
      "Epoch 43/50\n",
      "251/251 [==============================] - 637s 3s/step - loss: 0.0341 - categorical_accuracy: 0.9824 - val_loss: 2.2553 - val_categorical_accuracy: 0.7276\n",
      "Epoch 44/50\n",
      "251/251 [==============================] - 637s 3s/step - loss: 0.0584 - categorical_accuracy: 0.9748 - val_loss: 1.8489 - val_categorical_accuracy: 0.7365\n",
      "Epoch 45/50\n",
      "251/251 [==============================] - 637s 3s/step - loss: 0.0292 - categorical_accuracy: 0.9840 - val_loss: 2.1218 - val_categorical_accuracy: 0.7306\n",
      "Epoch 46/50\n",
      "251/251 [==============================] - 638s 3s/step - loss: 0.0271 - categorical_accuracy: 0.9846 - val_loss: 2.1110 - val_categorical_accuracy: 0.7390\n",
      "Epoch 47/50\n",
      "251/251 [==============================] - 638s 3s/step - loss: 0.0251 - categorical_accuracy: 0.9856 - val_loss: 2.1356 - val_categorical_accuracy: 0.7400\n",
      "Epoch 48/50\n",
      "251/251 [==============================] - 640s 3s/step - loss: 0.0228 - categorical_accuracy: 0.9864 - val_loss: 2.2137 - val_categorical_accuracy: 0.7415\n",
      "Epoch 49/50\n",
      "251/251 [==============================] - 639s 3s/step - loss: 0.0216 - categorical_accuracy: 0.9866 - val_loss: 2.2905 - val_categorical_accuracy: 0.7400\n",
      "Epoch 50/50\n",
      "251/251 [==============================] - 639s 3s/step - loss: 0.0214 - categorical_accuracy: 0.9865 - val_loss: 2.3322 - val_categorical_accuracy: 0.7405\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=test_ds,\n",
    "    epochs=50,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_model\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save('my_model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
